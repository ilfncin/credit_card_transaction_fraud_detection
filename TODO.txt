
********** **Pend√™ncia**: 

- Avaliar um conjunto reduzido. Usar o conjunto abaixo
features_selecionadas = ['amt_scaled', 'city_pop_scaled', 'gender_encoded_scaled', 'category_freq_scaled', 'cc_num_freq_scaled', 'age_scaled', 'unix_time_scaled']
X_sample_reduced = df_sample[features_selecionadas]
print(X_sample_reduced.info())


- nas an√°lises das m√©tricas, ao inves de trazer a m√°xima propor√ß√£o que pode ser enganaso, trazer um array, com v√°rias m√©tricas para cada cluster. mx proporcao, tamanho do cluster, recall, F1 etc. 
Pois pode haver um cluster com 100% mas s√≥ ter um duas transa√ß√µes e deixo de visualizar um outro cluster com 30% para com 1.000 transa√ß√µes, por exemplo







ver o video da aula de kleber sobre o optuna e tentar usar




rodar o kmeans v√°rias vezes partindo de posi√ß√µes aleatorias diferentes, pois isso pode impactar no resultado
----Como o algoritmo K-m√©dias encontra um √≥timo local ao inv√©s de um √≥timo global, ent√£o os resultados obtidos ir√£o depender das atribui√ß√µes (aleat√≥rias) das observa√ß√µes aos K clusters iniciais.
-------Portanto, √© importante rodar o algoritmo v√°rias vezes, partindo de configura√ß√µes iniciais (aleat√≥rias) diferentes. E ent√£o, selecionar a melhor solu√ß√£o, isto √©, aquela para a qual o valor da fun√ß√£o-objetivo √© menor.

procurar o uso do kmeans com kernel!

DBSCAN
rodar o dbscan em diferentes conficgura√ß√µes de Œµ e MinPts ; Use um gr√°fico k-dist plot: plote as dist√¢ncias dos k-vizinhos mais pr√≥ximos (onde k = MinPts) e procure o "joelho" da curva para estimar Œµ.
rodar os demais algoritmos estudados

Se houver muitos -1, considere:
Diminuir eps (dist√¢ncia de vizinhan√ßa).
Diminuir min_samples (quantidade de vizinhos para formar um cluster).
Verificar a escala dos dados (padroniza√ß√£o/normaliza√ß√£o pode impactar fortemente).
Explorar a distribui√ß√£o dos dados visualmente.


SOM

rodar o SOM com diferentes configura√ß√µes de hiperparametros. criar uma grande matriz e verificar o que acontece com as fraudes
----Hiperpar√¢metros Importantes
--------Tamanho da Grade (x, y): Determina a resolu√ß√£o do mapa. Grades maiores podem capturar mais nuances, mas exigem mais dados e computa√ß√£o.
--------Sigma: Controla a extens√£o da vizinhan√ßa durante o treinamento.
--------Taxa de Aprendizado: Define a magnitude das atualiza√ß√µes dos pesos.
--------N√∫mero de Itera√ß√µes: Quantidade de vezes que o algoritmo percorre o conjunto de dados.

rodar autoencoders
rodar os algoritmos de detec√ß√£o de fraudes da literatura


AVALIACAO
-Usar o optimalK implementado no arquivo da aula que o usa o √≠ndice gap


migrar as fun√ß√µes √∫teis para um arquivo python para ser reutilizada. Verificar um nome adequado para uma classe  e checar se outras fun√ß√µes poderiam ser criadas

Nesse dataset considerar que:


FEATURES: Nome do comerciante e nomes do cliente
- verificar como esses nomes podem influenciar no comportamento fraudulento
- Ver abordagens em 0.02-ilfn-data-exploration.ipynb e continuar novas an√°lises
- ver documento: https://docs.google.com/document/d/1y9KUw4tRHKMl2RRe4Kg8lK9le6hyjkCNojl9TOSfjS0/edit?tab=t.0
    - Frequ√™ncia acumulada at√© o momento da transa√ß√£o (sem vazamento de dados)
    - Frequ√™ncia acumulada em uma janela de tempo (exemplo: √∫ltimas 24 horas)

- O nome do cliente pode influenciar no comportamento fraudulento
- calcular a dist√¢ncia entre o cliente e o comerciante (a partir da latitude e longitude) pode ser interessante
- refatorar as fun√ß√µes que s√£o comuns para um arquivo .py
- avaliar o modelo com e sem a feature gender


FEATURE JOB
Analises para a feature job (procurar por esses termos no chatgpt):
- Distribui√ß√£o de Frequ√™ncia	Verificar se a vari√°vel segue uma distribui√ß√£o de cauda longa (alta cardinalidade com poucos cargos concentrando a maioria das transa√ß√µes).
- An√°lise de Outliers de Volume	Identificar cargos que, mesmo sendo poucos, concentram um volume desproporcional de transa√ß√µes (potencialmente interessantes para investiga√ß√£o de fraudes ou comportamentos at√≠picos).
- Percentual de Transa√ß√µes por Cargo	Entender o peso de cada cargo nas transa√ß√µes. Exemplo: Cargos com poucos representantes mas grande volume de compras.
- Correla√ß√£o com Outras Vari√°veis	Analisar se certos cargos tendem a concentrar transa√ß√µes de alto valor (amt), dist√¢ncias maiores (distance_km), ou maior incid√™ncia de transa√ß√µes online (category).
- An√°lise de Diversidade de Categorias por Cargo	Verificar se determinados cargos concentram transa√ß√µes em poucas ou muitas categorias comerciais, o que pode indicar perfis comportamentais distintos.

Implica√ß√µes para a Detec√ß√£o de Fraudes:
√â recomend√°vel realizar o agrupamento dos cargos com base nesses n√≠veis de atividade, criando uma nova vari√°vel categ√≥rica como job_activity_level (Exemplo: Baixa, M√©dia, Alta).
Os grupos com alta atividade (acima de 1000 transa√ß√µes) devem ser monitorados mais de perto, j√° que concentram a maior parte das opera√ß√µes e s√£o mais relevantes na detec√ß√£o de padr√µes an√¥malos.
Cargos com baixa atividade podem ser consolidados em uma √∫nica categoria (‚ÄúOutros‚Äù), reduzindo a dimensionalidade e evitando a introdu√ß√£o de ru√≠do nos algoritmos de clusteriza√ß√£o.

üí° Recomenda√ß√£o Final:
Utilizar t√©cnicas como Frequency Encoding ou Impact Encoding para manter a informa√ß√£o relevante da feature job.
Avaliar o cruzamento desses grupos de cargos com outras vari√°veis cr√≠ticas, como valor das transa√ß√µes (amt), dist√¢ncia entre cliente e comerciante (distance_km) e percentual de transa√ß√µes online, para identificar perfis de maior risco.